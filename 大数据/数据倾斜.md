[toc]

---


[source](https://www.bilibili.com/video/BV1jt4y1q7uW/?spm_id_from=333.788.videocard.2)


# Mapreduce数据倾斜

**什么是数据倾斜？**

数据中不可避免地会出现离群值（outlier）并导致数据倾斜。这些离群值会显著地拖慢 MapReduce 的执行

如：
node01 node02 node03 三个节点
node01 :90%
node02 :9%
node03 :1% 
则 node01 会拖慢整个 mapreduce 的执行任务

常见的数据倾斜有以下几类
- 数据頻率倾斜：某一个区域的数据量要远远大于其他区域。比如某一个 key 对应的键值对远远大于其他键的键值对。
- 数据大小倾斜：部分记录的大小远远大于平均值
  
在 map 端和 reduce 端都有可能发生数据倾斜
- 在 map 端的数据倾斜可以考虑使用 combine (一个 reduce 操作)
- 在 reduce 端的数据倾斜常常来源于 MapReduce 的默认分区
  
数据倾斜会导致 map 和 reduce 的任务执行时问大为延长，也会让需要缓存数据集的操作消耗更多的内存资源

## 如何诊断是否存在数据倾斜

**如何诊断哪些键存在数据倾斜？**

- 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。
有一个简便方法就是在代码里实现 **追踪每个键的最大值**。

- 为了減少追踪量，可以设置数据量阀值，只**追踪那些数据量大于阀偵的键**，并输出到日志中。实现代码如下 [(05:29)](https://www.bilibili.com/video/BV1jt4y1q7uW/?spm_id_from=333.788.videocard.2)
- 运行作业后就可以从目志中判断发生倾斜的键以及倾斜程度：**跟踪倾斜数据** 是了解数据的重要一步，也是设计 Mapreduce作业的重要基础


## 减缓数据倾斜

当从打印的 log 中知道了经常会出现数据倾斜的情况，就可以针对这些常见的情况进行优化来减少数据倾斜的发生。

Reduce 数据倾斜一般是指 map 的输出数据中存在数据頻率倾斜的状况，
即 **部分输出键的数据量远远大于其它的输出键**

**如何减小 reduce 端数据倾斜的性能损失？** 
常用方式有：
### 1、**自定义分区**
**基于输出键的背景知识进行自定义分区**

例如，如果 map 输出键的单词来源于一本书。
其中大部分必然是省略词（stopword）。
那么就可以用自定义分区将这大部分省略词发送给**固定的一部分 reduce 实例**。
而将其他的都发送给剩的 reduce 实例

看到 [09:52](https://www.bilibili.com/video/BV1jt4y1q7uW/?spm_id_from=333.788.videocard.2)


### 2、Combine
使用 Combine 可以大量地減小数据频率倾斜和数据大小倾斜 
Combine 的目的是 **聚合并精简数据**。

### 3、抽样和范围分区
Hadoop 默认的分区器是 Hashpartitioner ，基于 map 输出健的哈希值分区。这仅在数据分布比较均匀时比较好。在有数据倾斜时就很有问题使用分区器需要首先了解数据的特性。** Tota10 rderpartitioner **中，可以通过对原始数据进行抽样得到的结果集来**预设分区边界值
Tota1 Orderpartitioner 中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题

### 4、数据大小倾斜的自定义策略
在 map 端或 reduce 端的数据大小倾斜都会对缓存造成较大的影响，乃至导致 Outofmemoryerror 异常。处理这种情况并不容易。可以参考以下方法设置 mapreduce. input.1 inerecordreader.1ine. maxlength 来限制 Recordreader 读取的最大长度。
Recordreader 在 Textinputformat 。和 Keyvaluetext Inputformat 类中使用。駅认长度没有上限。



