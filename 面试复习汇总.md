[toc]


---



# **python 异常机制**

```py
try:
    # ...
except (Exception1, Exception2) as e:
    # 异常处理
else:
    # 异常没发生时处理
finally:
    # 无论有无异常都会执行 例如资源关闭和释放
```

## 异常类关系
。。。
<br>
<br>
<br>


## 自定义异常

```py
# 自定义异常, 继承自 Exception 类
class MyException(Exception):
    pass

try:
    raise MyException('error massagr')
except MyException as e:
    print(e)
```

---------------------------------------------------------

# **python 2。3 差异**

## python3 新增特性

| 差异 <a id="dif_top"><a> |python3 | python2
| :--- |:---|:---
| print | 成为一个函数 print() | print 只是一个关键字
| <a href="#1">编码问题</a>  | 3 中不再有 Unicode 对象，str 默认就是 unicode | 
|<a href="#2">除法</a>   |  单斜杠`/`能整除得到小数，双斜杠`//`能整除得到整数  | 只能得到整数的商
| <a href="#3">类型注解</a>  | 可以给变量加类型提示（zhuyi注意只是提示,要进行类型检查需要借助 IDE 工具  |
| <a href="#4">优化 `super()`函数</a>  |   |
|  <a href="#5">解包操作</a> |   |
|  关键字参数 （这个我会了，不写了 |   |
| <a href="#6">Chained exceptions. Python3 重新抛出异常信息不会丢失栈信息</a>  |   |
| <a href="#7">一切返回迭代器<br> `range, zip, map, dict.values ...` </a>  |   |

## python3 新增内置库
| 差异 <a id="ku_top"><a> |python3 | python2
| :--- |:---|:---
| yield from 链接子生成器  |   |
|  asyncio 内置库， asyic/await 原生协程支持异步编程 |   |
| 新内置库 `enum, mock, asyncio, ipaddess, concurrent.futrues 等`  |   |
|   |   |


- **<a href="#dif_top">编码问题 <a id="1"><a>**



```py
# python3:
s = '中文'       # 不用加 u 前缀
print(typr(s))  # str， str默认就是unicode

def 大写(s):return s.upper()    # 甚至可以用中文作为函数名
print(大写('abc'))

# -----------------------------
# python2:
s = u'中文'     # 要加 u 前缀
print(typr(s))  # unicode
```


- **<a href="#dif_top">除法 <a id="2"><a>**

```py
# python3:
5/2     # 2.5
5//2    # 2

# python2:
5/2     # 2
```



- **<a href="#dif_top">类型注解 <a id="3"><a>**
```py
# python3:
# 参数 name 是 str 类型，返回一个 str 类型
def hello(name: str) -> str:
    return 'hello'+name
```




- **<a href="#dif_top">优化 `super()`函数 <a id="4"><a>**

```py
# 父类：
class Base(object):
    def hello(self):
        print('hello')

# python3:
Class C1(Base):
    def hello(self):
        return super().hello()  # 引用父类

# python2:
Class C2(Base):
    def hello(self):
        return super(C, self).hello()  # 引用父类
```



- **<a href="#dif_top">解包操作 <a id="5"><a>**

```py
# python3:
a, b, *c = range(10)
print(a)    # 0
print(b)    # 1
print(c)    # [2, 3, 4, 5, 6, 7, 8, 9]

a, b, *_ = range(5)    # 提取序列中前面两个元素，其他的舍弃

```


- **<a href="#dif_top">Chained exceptions. Python3 重新抛出异常信息不会丢失栈信息 <a id="6"><a>**


```py
# python3:


# python2:

```


- **<a href="#dif_top">一切返回迭代器 `range, zip, map, dict.values ...` <a id="7"><a>**

```py
# python3:
range(10)   # 输出: range(0, 10) 迭代器，懒加载, 即不一次性全部加载出来，你要哪一个才给你哪一个，优化性能

# python2:
range(10)   # 生成: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 列表，占内存
```

---------------------------------------------------------

# **常用内置算法与数据结构**
| 数据结构 / 算法 <a id="inside_suanfa"><a> |python 语言内置 | 内置库
| :--- |:---|:---
| 线性结构 | list, tuple |  array(数组，不常用) / `collections.namedtuple`
| 链式结构 |  | `collections.deque` (双端队列)
| 字典结构 |   |`collections.Counter` (计数器) / `OrderedDict` (有序字典)
| 集合结构 | set (集合) / `frozenset` (不可变集合) |
| **算法** |   |
| 排序算法 | <a href="#sorted">`sorted`  |
| 二分算法 |   |  `bisect` 模块
| 堆算法 |   | `heapq` 模块
| 缓存算法 |   |`functools.lru_cache`(Least Recent Used,python)




- **<a href="#inside_suanfa">sorted <a id="sorted">**
```py

```


---------------------------------------------------------



## collections 模块

TODO



---------------------------------------------------------

# **排序**

[排序动画](https://zhuanlan.zhihu.com/p/52884590)

## 1. 插入排序 √
![插入排序](https://pic4.zhimg.com/v2-6e67d1c722106442b422ee53e98575b3_b.gif)

遍历列表中每一数，每个数一次跟列表其他数比较一遍，放在合适的位置。
找到合适的位置时，其后面的数会都向后挪出一个位置给此时正在比较的数
时间复杂度: O(n^2)
```py
# 直接插入排序
def insertSort(lis):
    for prp_idx in range(1, len(lis)):# prp_idx: 红色的那个数的索引
        for j in range(prp_idx):      # 黑色的列表里面的数
            if lis[prp_idx] < lis[j]: # 红色的那个数 和 黑色的列表里面的数全部比一遍
                lis.insert(j, lis[prp_idx]) # 新增一个数 (所以后面要删除一个数)
                lis.pop(prp_idx + 1)  
                break
    return lis

print insertSort([1,5,2,6,9,3])
```

## 2. 希尔排序（Shell Sort）
插入排序的修改版。
根据步长由长到短分组，进行排序，直到步长为1为止，属于插入排序的一种。
![image](http://images2015.cnblogs.com/blog/318837/201604/318837-20160422102024757-37862627.png)
```py
#引用网址：http://www.cnblogs.com/qlshine/p/6052223.html
def shellSort(nums):    # 没看懂
    # 设定步长
    step = len(nums)/2
    while step > 0:
        for i in range(step, len(nums)):
            # 类似插入排序, 当前值与指定步长之前的值比较, 符合条件则交换位置
            while i >= step and nums[i-step] > nums[i]:
                nums[i], nums[i-step] = nums[i-step], nums[i]
                i -= step
        step = step/2
    return nums
```

## 3. 选择排序 √
```py
比如在一个长度为 N 的无序数组中：

第 1 趟  : 遍历 N 个数据，找出其中最小的数值与第一个元素交换，
第 2 趟  : 遍历剩下的 N-1 个数据，找出其中最小的数值与第二个元素交换
......
第 N-1 趟: 遍历剩下的 2 个数据，找出其中最小的数值与第N-1个元素交换，至此选择排序完成。
```
```py
以下面5个无序的数据为例：
56 12 80 91 20（文中仅细化了第一趟的选择过程）

第 1 趟：12 56 80 91 20
第 2 趟：12 20 80 91 56
第 3 趟：12 20 56 91 80
第 4 趟：12 20 56 80 91
```
![image](https://img-my.csdn.net/uploads/201208/28/1346124560_3555.jpg)

```py
def select_sort(lis):
    for i in range(len(lis)-1): # 要进行多少轮
        min = i     # 记录当前正在进行比较的那个数的索引 假设为最小值
        for j in range(i+1, len(lis)): # 当前那个数 与 后面的数比较
            if lis[j] < lis[min]:
                min = j
        lis[i], lis[min] = lis[min], lis[i]
```
选择排序比冒泡排序时间上要优秀一点，主要在于，每一轮冒泡排序每次比较都需要交换位置，而选择排序每一轮通过记录最大或最小位置的索引最后只交换一次位置就可以了

## 4. 基数排序（Radix Sort）

## 5. 冒泡排序 √
![冒泡排序](https://pic4.zhimg.com/v2-d4c88b8cc620af6af67c33910899fcf7_b.gif)

每一轮都依次选择两个元素。
没排序好就再进行下一轮，继续两两比较，直到排序好。
```py
def bubbleSort(lis):
    for i in range(len(lis)-1):         # 要进行多少轮
        for j in range(len(lis)-i-1):   # 每一轮的比较范围 len(lis)—i-1:后面的黑色的已经排序好的不再继续比较
            if lis[j] > lis[j+1]:       # 比较范围内两两比较
                lis[j], lis[j+1] = lis[j+1], lis[j]
    return lis
```

## 6. 归并排序（Merge Sort）
![归并排序](https://pic4.zhimg.com/v2-a29c0dd0186d1f8cef3c5ebdedf3e5a3_b.gif)

```py
# 归并排序
#   思路：分治法 三步走
#   1. 分       : 以一个数作为基准 (这里用中间值)，将原序列分开为 两个子序列
#   2. 排(递归) : 对两个子序列一直重复步骤 1, 直到每个
#   3. 合并     :

# 合并两个有序序列 实现
def merge_sorted_list(seq1, seq2):
    len1, len2 = len(seq1), len(seq2)
    i = j = 0   # 分别做为 seq1, seq2 的索引
    ret_seq = []
    
    while i < len1 and j < len2:    # 步骤 1, 2
        if seq1[i] < seq2[j]:
            ret_seq.append(seq1[i]) # 注意没有从原序列剔除元素，只是复制到 ret_seq
            i += 1
        else:
            ret_seq.append(seq2[j])
            j += 1
    
    if i < len1:                    # 步骤 3
        ret_seq.extend(seq1[i:])
    else:
        ret_seq.extend(seq2[j:])
    
    return ret_seq


# 【 归并排序 】 实现
def merge_sort(seq):
    if len(seq) <= 1:
        return seq              # 递归出口
    else:
        mid = int(len(seq)/2)   # 中间的数作为基准数 mid: 基准数索引
        left_half = merge_sort(seq[:mid])
        print('left_half :{}'.format(left_half))
        right_half = merge_sort(seq[mid:])
        print('rght_half :{}'.format(right_half))
        # 到这里时 left_half, right_half 分别都是有序序列
        return merge_sorted_list(left_half, right_half)

# 归并排序 测试用例
def test_merge_sort():
    import random
    test_seq = list(range(10))
    random.shuffle(test_seq)    # 打乱顺序
    # print(test_seq)
    assert merge_sort(test_seq) == sorted(test_seq)  # 没报错就没错
    # print(merge_sort(test_seq))

if __name__ == '__main__':
    test_merge_sort()
```


## 7. 堆排序（Heap Sort）
堆排序是一种基于二叉堆（Binary Heap）结构的排序算法，
所谓二叉堆，我们通过完全二叉树来对比，只不过相比较完全二叉树而言，二叉堆的所有父节点的值都大于（或者小于）它的孩子节点，像这样：
![堆排序](https://pic2.zhimg.com/80/v2-5d6119c9801eadb83402ef68f6d4b689_hd.jpg)
![堆排序](http://images2015.cnblogs.com/blog/318837/201604/318837-20160422104522991-406805984.png)

首先需要引入最大堆的定义：
- 最大堆中的最大元素值出现在根结点（堆顶）
- 堆中每个父节点的元素值都大于等于其孩子结点

堆排序的方法如下，把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。

## 8. 桶排序（Bucket Sort）

## 9. 快速排序（Quick Sort）√
![快排](https://img-blog.csdn.net/20170731201511649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTXJMZXZvNTIw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```py
def quicksort(Sep, first, last):    # average: O(nlog(n))
    if first < last:
        new_base = partitionSeq(Sep, first, last)
        quicksort(Sep, first, new_base-1)   # 对划分的子数组递归操作
        quicksort(Sep, new_base+1, last)

def partitionSeq(Sep, first, last):
    """ 快排中的划分操作，把比 base 小的挪到左边，比 base 大的挪到右边"""
    # 注意 base 是值，left right 是索引
    base = Sep[first]   # 以待划分的序列的第一个元素作为基准数
    left = first + 1    # 从第二个元素索引开始
    right = last

    while True:
        while left <= right and Sep[left] < base:
            left += 1
        # 此时 left 为从左边开始 第一个比 base 大的元素的索引 
        # ---------------------------------
        while right >= left and Sep[right] >= base:
            right -= 1
        # 此时 right 为从右边开始 第一个比 base 小的元素的索引 
        # ---------------------------------
        if left < right:    # 索引 left 在 索引 right 前面
            Sep[left], Sep[right] = Sep[right], Sep[left]   # 元素对调
        else:               # 索引 right 在 索引 left 前面
            break

    # 把 base 和 索引right 位置对调
    Sep[first], Sep[right] = Sep[right], Sep[first]
    return right    # 返回对调后的 base 位置

```
## 快速排序 *
(运行 快速排序.py 文件可以查看结果)
```py
# 快速排序
# 思路：递归
#   1. 从 序列 seq 中选第一个参数作为基准数 base
#   2. 以 base 作为基准比较 seq, 比 base 小的 left_part 放 base 前面，比 base 大的 right_part 放 base 后面
#   3. left_part, right_part 分别重复 1.2 步骤

# 快速排序实现
def quicksort(seq):
    if len(seq) < 2:
        return seq    # 递归出口
    else:
        base = seq[0]
        left_part = [i for i in seq[base_idx+1:] if i<= base]
        right_part = [i for i in seq[base_idx+1:] if i > base]
    return quicksort(left_part) + [base] +quicksort(right_part)   # 递归 合并

# 测试用例
def test_quicksort():
    import random
    test_seq = list(range(10))
    random.shuffle(test_seq)    # 打乱顺序
    # print(test_seq)
    assert quicksort(test_seq) == sorted(test_seq)  # 没报错就没错

# 运行测试用例
test_quicksort()
```
图解：
![快排](https://img-blog.csdn.net/20170731201511649?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTXJMZXZvNTIw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


## 合并两个有序序列
(运行 合并两个有序序列.py 文件可以查看结果)
```py
# 合并两个有序序列
#   有两个有序序列分别为 seq1, seq2。 给一个空序列 ret_seq 放最后排序好的结果
#   seq1, seq2 索引分别为 i, j, 索引值初始化为 0
#   思路：循环 索引推移
#       1. seq1[i] 与 seq2[j] 比较，小的那个值放入 ret_seq, 索引+1向后推移
#       2. 重复 1 步骤，直到 seq1, seq2 其中一个序列的索引走到最后
#       3. 索引没有走到最后的另一序列直接加到 ret_seq 后面 (已经排序好了的可以直接加)

# 实现
def merge_sorted_list(seq1, seq2):
    len1, len2 = len(seq1), len(seq2)
    i = j = 0   # 分别做为 seq1, seq2 的索引
    ret_seq = []
    
    while i < len1 and j < len2:    # 步骤 1, 2
        if seq1[i] < seq2[j]:
            ret_seq.append(seq1[i]) # 注意没有从原序列剔除元素，只是复制到 ret_seq
            i += 1
        else:
            ret_seq.append(seq2[j])
            j += 1
    
    if i < len1:                    # 步骤 3
        ret_seq.extend(seq1[i:])
    else:
        ret_seq.extend(seq2[j:])
    
    return ret_seq

# 测试
a = [1, 2, 5]
b = [0, 6, 7, 2, 3, 9]
# 注意给的参数序列要有序
print(merge_sorted_list(sorted(a), sorted(b)))
```
图解：
![归并]()       TODO




## 归并排序 *
(运行 归并排序.py 文件可以查看结果)
```py
# 归并排序
#   思路：分治法 三步走
#   1. 分       : 以一个数作为基准 (这里用中间值)，将原序列分开为 两个子序列
#   2. 排(递归) : 对两个子序列一直重复步骤 1, 直到每个
#   3. 合并     :

# 合并两个有序序列 实现
def merge_sorted_list(seq1, seq2):
    len1, len2 = len(seq1), len(seq2)
    i = j = 0   # 分别做为 seq1, seq2 的索引
    ret_seq = []
    
    while i < len1 and j < len2:    # 步骤 1, 2
        if seq1[i] < seq2[j]:
            ret_seq.append(seq1[i]) # 注意没有从原序列剔除元素，只是复制到 ret_seq
            i += 1
        else:
            ret_seq.append(seq2[j])
            j += 1
    
    if i < len1:                    # 步骤 3
        ret_seq.extend(seq1[i:])
    else:
        ret_seq.extend(seq2[j:])
    
    return ret_seq


# 【 归并排序 】 实现
def merge_sort(seq):
    if len(seq) <= 1:
        return seq              # 递归出口
    else:
        mid = int(len(seq)/2)   # 中间的数作为基准数 mid: 基准数索引
        left_half = merge_sort(seq[:mid])   # 进到最里面的那扇门结果就是每个子序列只有一个元素 然后出来往下走
        right_half = merge_sort(seq[mid:])
        # 到这里时 left_half, right_half 分别都是有序序列
        return merge_sorted_list(left_half, right_half)

# 归并排序 测试用例
def test_merge_sort():
    import random
    test_seq = list(range(10))
    random.shuffle(test_seq)    # 打乱顺序
    # print(test_seq)
    assert merge_sort(test_seq) == sorted(test_seq)  # 没报错就没错
    # print(merge_sort(test_seq))

if __name__ == '__main__':
    test_merge_sort()
```
图解：
![归并排序](https://pic4.zhimg.com/v2-a29c0dd0186d1f8cef3c5ebdedf3e5a3_b.gif)



------------------------------------------


# **查找**

## 二分查找

```py
# 二分查找
#   思路：循环
#   每次都索引 /2 得到中间值比较目标值，并根据大小缩小一半的查找范伟

# 经常会让手写二分查找，要注意边界（其实 python 有个 bisect 模块 ）


# 【 二分查找 】 实现 (注意待查找序列要是有序序列)
def binary_serach(sorted_seq, val):
    if not sorted_seq:
        return -1
    
    begin = 0
    end = len(sorted_seq) - 1
    while begin <= end:
        mid = int((begin + end) / 2)    # int()屏蔽 2/3 差异
        if sorted_seq[mid] == val:
            return mid
        elif sorted_seq[mid] > val:
            end = mid - 1
        else:
            begin = mid + 1
    return -1

# 二分查找 测试用例
def test_binary_serach():
    import random
    test_seq = range(10)
    i = random.choice(test_seq)
    assert binary_serach(test_seq, i) == i

if __name__ == '__main__':
    test_binary_serach()
```

---------------------------------------------------------

# **哈希 Hash (散列)**

[哈希函数，解决哈希冲突](http://note.youdao.com/noteshare?id=e181d7c3684ddf0661c23450eeeaae20&sub=C1D3097502EC4D7CABBA55931B398BD3)

原文：
https://www.jianshu.com/p/de33dc676a3f

## Hash 表结构
 
Hash表采用一个映射函数 f :<br>
==key —> address==<br>
将 key 关键字映射到该记录在表中的存储位置，<br>

从而在想要查找该记录时，可以直接根据==key关键字和**值的地址**的映射关系==得到该记录在表中的存储位置，<br>
通常情况下，这种 **映射关系称作为Hash函数**，<br>
而通过Hash函数和 key 关键字计算出来的存储位置(注意这里的存储位置只是表中的存储位置，并不是实际的物理地址)称作为 **Hash地址**。<br>

比如上述例子中，假如联系人信息采用Hash表存储，<br>
则当想要找到“李四”的信息时，直接根据“李四”和Hash函数计算出Hash地址即可

## Hash 表设计
Hash函数设计的好坏直接影响到对Hash表的操作效率。

### 错误例子
下面举例说明：
假如对上述的联系人信息进行存储时，
```
采用的 Hash 函数为：姓名的每个字的拼音开头大写字母的ASCII码之和:
    address(张三) = ASCII(Z)+ASCII(S) = 90+83 = 173;
    address(李四) = ASCII(L)+ASCII(S) = 76+83 = 159;
    address(王五) = ASCII(W)+ASCII(W) = 87+87 = 174;
    address(张帅) = ASCII(Z)+ASCII(S) = 90+83 = 173;
```
假如只有这4个联系人信息需要进行存储，这个Hash函数设计的很糟糕
- 它浪费了大量的存储空间。因为假如采用`char`型数组存储联系人信息的话，每个人的信息需要12个字节来存储
- 冲突。address(张三)和address(张帅)具有相同的地址,只需要存储4条记录就发生了冲突


### 一、几种构造 Hash 函数
#### 1. 直接定址法 (线性函数)

取 key 关键字或者关键字的某个 **==线性函数==** 为 Hash 地址，<br>即 **`address(key)=a*key+b`**;<br>

如知道学生的学号从2000开始，最大为4000，<br>则可以将 **`address(key) = key-2000`** 作为Hash地址。


#### 2. 平方取中法

- 对 key 关键字进行 **==平方运算==**，
- 然后 **==取结果的中间几位==** 作为 Hash 地址。<br>

 假如有以下 key 关键字序列 {421，423，436}，<br>平方之后的结果为 {177241，178929，190096}，<br>那么可以**取中间的两位数** **{72，89，00}** 作为Hash地址。

#### 3. 折叠法

- 将 key 关键字 **==拆分==** 成几部分，
- 然后将这几部分以 **==特定的方式组合==** 在一起， 进行转化形成Hash地址。<br>

假如知道图书的 ISBN 号为 8903-241-23，<br>可以将 **`address(key) = 89 + 03 + 24 + 12 + 3`** 作为Hash地址。

#### 4. 除留取余法
如果知道 Hash 表的最大长度为m，可以取 **==不大于m的最大质数 p==** ，<br>然后对 key 关键字进行 **==取余==** 运算。
**`address(key) = key % p`** <br>

在这里p的选取非常关键，p选择的好的话，能够最大程度地减少冲突，<u>p一般取不大于m的最大质数</u> 。

### 二、Hash 表大小的确定






### 三、解决冲突
**冲突** : 
通过 Hash 函数得到的地址集合中 有两个以上是相同的，
那么通过 key 查找是就会造成 不同的值查找到的同一个地址。
但一个地址有两个不一样的值，即产生冲突。


虽然能够采用一些办法去减少冲突，但是冲突是无法完全避免的。因此需要根据实际情况选取解决冲突的办法。

#### 1. 开放定址法（探测法

即当一个 key 关键字和另一个 key 关键字发生冲突时，
使用某种探测技术在Hash表中形成一个探测序列，
然后沿着这个探测序列继续依次查找下去，
**==当碰到一个空的单元时，则插入其中==**。

比较常用的探测方法有 <u> **线性探测法**</u>，
比如有一组 key 关键字 
`{12，13，25，23，38，34，6，84，91}` ，
Hash 表长为14，Hash 函数为 **`address(key)= key % 11`** ，
当插入`12，13，25`时可以直接插入，
而当插入`23`时，地址 1 被`12`占用了，
因此沿着地址1依次往下探测 (探测步长可以根据情况而定)，
直到探测到地址 4，发现为空，则将`23`插入其中。


#### 2. 链地址法

采用 **==数组和链表相结合==** 的办法，
将 Hash 地址相同的记录存储在一张线性表中，而每张表的表头的序号即为计算得到的 Hash 地址

dino 理解：**假如有两个以上的 key 通过 哈希函数得到同一个地址，那就在那个地址上延伸线性结构，让那个地址能够存储两个以上元素的地址**

![链地址法](https://upload-images.jianshu.io/upload_images/6946981-697dd755189f9d65.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/579/format/webp)

![test](https://github.com/yesdino/img_upload/blob/master/imooc_study/hash/ResolveConflict_LinkMethod.png?raw=true)











<br><br><br><br><br><br>

这篇文章还有很多值得写的地方 有空可以继续学下去






<br><br><br>
作者：海天一树X
链接：https://www.jianshu.com/p/de33dc676a3f
来源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。


### 哈希表冲突与扩容规则

TODO

----------------------------------

[原文](https://mp.weixin.qq.com/s?__biz=MjM5OTMyODA4Nw==&mid=2247484084&idx=1&sn=573989b9526aef01a3d515ab09afe86a&chksm=a73c628c904beb9a39adef9b95a1ce6560245b7f4e2a39207a55abc1a293935be203a35bcb13&scene=21#wechat_redirect)



进程(process)和线程(thread)是非常抽象的概念, 也是程序员必需掌握的核心知识。多进程和多线程编程对于代码的并发执行，提升代码效率和缩短运行时间至关重要。小编我今天就来尝试下用一文总结下Python多进程和多线程的概念和区别, 并详细介绍如何使用python的multiprocess和threading模块进行多线程和多进程编程。


# **重要知识点 - 什么是进程(process)和线程(thread)**

|进程   |线程   |
|----   |----   |
<u>操作系统分配资源</u>的最小单元     | <u>操作系统调度</u>的最小单元。
一个应用程序至少包括1个进程   | 1个进程包括1个或多个线程，线程的尺度更小。
每个进程在执行过程中拥有<u>独立的内存单元</u> | 一个线程的多个线程在执行过程中<u>共享内存</u>。

<br>
网上有篇阮一峰的博客曾对进程和线程做出了一个非常浅显的解释，我在这里贴出来方便大家理解。

-  计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。

- 假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。<br>背后的含义就是，单个CPU一次只能运行一个任务。<br>编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。

- ==进程就好比工厂的车间==，它代表CPU所能处理的单个任务。<br>任一时刻，一个CPU总是运行一个进程，其他进程处于非运行状态。

- 一个车间里，可以有很多工人。他们协同完成一个任务。

- ==线程就好比车间里的工人==。一个进程可以包括多个线程。

- 车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。<br>这象征<u>一个进程的内存空间是共享的，每个线程都可以使用这些共享内存</u>。

- 可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。<br>这代表<u>一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存</u>。

- 一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。<br>这就叫<u>"==互斥锁=="(Mutual exclusion,缩写 Mutex)，防止多个线程同时读写某一块内存区域</u>。

- 还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。<br>这好比<u>某些内存区域，只能供给固定数目的线程使用</u>。

- 这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。<br>这种做法叫做<u>"==信号量=="(Semaphore)，用来保证多个线程不会互相冲突</u>。

不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。


原文地址见
http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html


# **多进程**
## multiprocess 模块

python的多进程编程主要依靠 **multiprocess** 模块。

我们先对比两段代码，看看多进程编程的优势。<br>我们模拟了一个非常耗时的任务，计算8的20次方，为了使这个任务显得更耗时，我们还让它sleep 2秒。<br>第一段代码是单进程计算(代码如下所示)，我们按顺序执行代码，重复计算2次，并打印出总共耗时。
```py
# 单进程计算
import time
import os

def long_time_task():
    print('当前进程: {}'.format(os.getpid()))# os.getpid():获取当前进程ID号
    time.sleep(2)
    print("结果: {}".format(8 ** 20))

if __name__ == "__main__":
    print('当前母进程: {}'.format(os.getpid()))
    start = time.time()
    for i in range(2):
        long_time_task()

    end = time.time()
    print("用时{}秒".format((end-start)))

# 输出结果如下，总共耗时4秒，至始至终只有一个进程14236。看来电脑计算8的20次方基本不费时。
# 当前母进程: 14236
# 当前进程: 14236
# 结果: 1152921504606846976
# 当前进程: 14236
# 结果: 1152921504606846976
# 用时4.01080060005188秒
```

第2段代码是多进程计算代码。
### Process() 创建进程
我们利用multiprocess模块的**Process**方法创建了两个新的进程p1和p2来进行并行计算。<br>Process方法接收两个参数, 第一个是target，一般指向函数名，第二个时args，需要向函数传递的参数。<br>对于创建的新进程，调用start()方法即可让其开始。<br>我们可以使用os.getpid()打印出当前进程的名字。
```py
from multiprocessing import Process
import os
import time


def long_time_task(i):
    print('子进程: {} - 任务{}'.format(os.getpid(), i))
    time.sleep(2)
    print("结果: {}".format(8 ** 20))


if __name__=='__main__':
    print('当前母进程: {}'.format(os.getpid()))
    start = time.time()
    p1 = Process(target=long_time_task, args=(1,))
    p2 = Process(target=long_time_task, args=(2,))
    print('等待所有子进程完成。')
    p1.start()      # start(): 进程开始
    p2.start()      
    p1.join()       # join(): 等待子进程结束主进程才可以结束
    p2.join()
    end = time.time()
    print("总共用时{}秒".format((end - start)))

# 输出结果如下所示，耗时变为2秒，时间减了一半，可见并发执行的时间明显比顺序执行要快很多。你还可以看到尽管我们只创建了两个进程，可实际运行中却包含里1个母进程和2个子进程。之所以我们使用join()方法就是为了让母进程阻塞，等待子进程都完成后才打印出总共耗时，否则输出时间只是母进程执行的时间。
# 当前母进程: 6920
# 等待所有子进程完成。
# 子进程: 17020 - 任务1
# 子进程: 5904 - 任务2
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 总共用时2.131091356277466秒
```

**知识点**:
- 新创建的进程与进程的切换都是要耗资源的，所以平时工作中进程数不能开太大。
- 同时可以 **运行的进程数一般受制于CPU的核数**。
- 除了使用Process方法，我们还可以使用Pool类创建多进程。

### Pool 类 创建进程
利用multiprocess模块的Pool类创建多进程

很多时候系统都需要创建多个进程以提高CPU的利用率，当数量较少时，可以手动生成一个个Process实例。<br>当进程数量很多时，或许可以利用循环，但是这需要程序员手动管理系统中并发进程的数量，有时会很麻烦。

这时进程池Pool就可以发挥其功效了。<br>可以通过传递参数限制并发进程的数量，默认值为CPU的核数。 

Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果 **==进程池==** 还没有满，就会创建一个新的进程来执行请求。<br>如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 

下面介绍一下multiprocessing 模块下的Pool类的几个方法：
#### apply_async()
```py
apply_async(func[, args=()[, kwds={}[, callback=None]]])
```
其作用是**向进程池提交需要执行的函数及参数**， 

各个进程采用 **非阻塞（异步）** 的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。

#### map()
```py
map(func, iterable[, chunksize=None])
```

与内置的map函数用法行为基本一致，它会 **使进程阻塞直到结果返回**。<br> 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。

#### map_async()
```py
map_async(func, iterable[, chunksize[, callback]])
```
与map用法一致，但是它是 **非阻塞** 的。其有关事项见apply_async。

#### close()
关闭进程池（pool），使其不在接受新的任务。

#### terminate()
结束工作进程，不在处理未处理的任务。

#### join()
主进程阻塞等待子进程的退出，<u>join方法要在close或terminate之后使用</u> 。

<br>
下例是一个简单的multiprocessing.Pool类的实例。因为小编我的CPU是4核的，一次最多可以同时运行4个进程，所以我开启了一个容量为4的进程池。

**4个进程需要计算5次**，你可以想象4个进程并行4次计算任务后，还剩一次计算任务(任务4)没有完成，**系统会等待4个进程完成后重新安排一个进程来计算**。

```py
from multiprocessing import Pool, cpu_count
import os
import time


def long_time_task(i):
    print('子进程: {} - 任务{}'.format(os.getpid(), i))
    time.sleep(2)
    print("结果: {}".format(8 ** 20))


if __name__=='__main__':
    print("CPU内核数:{}".format(cpu_count()))
    print('当前母进程: {}'.format(os.getpid()))
    start = time.time()
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('等待所有子进程完成。')
    p.close()
    p.join()  # 调用join()之前必须先调用close()或terminate()方法，让其不再接受新的Process了
    end = time.time()
    print("总共用时{}秒".format((end - start)))

# 5个任务（每个任务大约耗时2秒)使用多进程并行计算只需4.37秒,, 耗时减少了60%。
# 
# 输出：
# CPU内核数:4
# 当前母进程: 2556
# 等待所有子进程完成。
# 子进程: 16480 - 任务0
# 子进程: 15216 - 任务1
# 子进程: 15764 - 任务2
# 子进程: 10176 - 任务3
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 子进程: 15216 - 任务4
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 总共用时4.377134561538696秒
```

知识点:  
对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()或terminate()方法，让其不再接受新的Process了。

---

相信大家都知道python解释器中存在 **GIL(全局解释器锁)** , 它的作用就是**保证同一时刻只有一个线程可以执行代码**。<br>由于GIL的存在，很多人认为python中的多线程其实并不是真正的多线程，如果想要充分地使用多核CPU的资源，在python中大部分情况需要使用多进程。<br>然而这并意味着python多线程编程没有意义哦，请继续阅读下文。

## 多进程间的通信交互与数据共享 *

通常，进程之间是相互独立的，每个进程都有独立的内存。

### 共享内存 (nmap模块)
- 通过**共享内存 (nmap模块)**，进程之间可以共享对象，使多个进程可以访问同一个变量(地址相同，变量名可能不同)。<br>多进程共享资源必然会导致进程间相互竞争，所以应该尽最大可能防止使用共享状态。

### 队列queue 做进程参数
- 还有一种方式就是使用**队列queue**来实现不同进程间的通信或数据共享，这一点和多线程编程类似。


下例这段代码中中创建了2个独立进程，一个负责写(pw), 一个负责读(pr), 实现了共享一个队列queue。
```py
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: {}'.format(os.getpid()))
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random()) # 延迟一个随机数

# 读数据进程执行的代码:
def read(q):
    print('Process to read:{}'.format(os.getpid()))
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    q = Queue()     # 父进程创建Queue，并作为参数传给各个子进程
    pw = Process(target=write, args=(q,))   # 线程 pw, 队列参数q
    pr = Process(target=read, args=(q,))    # 线程 pr, 队列参数q
    
    pw.start()      # 启动子进程pw 写入
    pr.start()      # 启动子进程pr 读取
    
    pw.join()       # 等待 pw 结束
    
    pr.terminate()  # pr进程里是死循环，无法等待其结束，只能强行终止

# 输出结果如下所示:
# Process to write: 3036
# Put A to queue...
# Process to read:9408
# Get A from queue.
# Put B to queue...
# Get B from queue.
# Put C to queue...
# Get C from queue.
```

# **多线程**
## threading 模块

python 3中的多进程编程主要依靠threading模块。
<u>创建新线程与创建新进程的方法非常类似。</u>

**threading.Thread** 方法可以接收两个参数, 第一个是target，一般指向函数名，第二个时args，需要向函数传递的参数。

对于创建的新线程，调用start()方法即可让其开始。
我们还可以 ==使用`current_thread().name`打印出当前线程的名字==。 

下例中我们使用多线程技术重构之前的计算代码。
```py
import threading
import time


def long_time_task(i):
    print('当前子线程: {} - 任务{}'.format(threading.current_thread().name, i))
    time.sleep(2)
    print("结果: {}".format(8 ** 20))


if __name__=='__main__':
    start = time.time()
    print('这是主线程：{}'.format(threading.current_thread().name))

    t1 = threading.Thread(target=long_time_task, args=(1,))
    t2 = threading.Thread(target=long_time_task, args=(2,))

    t1.start()
    t2.start()

    end = time.time()
    print("总共用时{}秒".format((end - start)))

# 为什么总耗时居然是0秒? 我们可以明显看到主线程和子线程其实是独立运行的，主线程根本没有等子线程完成，而是自己结束后就打印了消耗时间。主线程结束后，子线程仍在独立运行，这显然不是我们想要的。

# 输出：
# 这是主线程：MainThread
# 当前子线程: Thread-1 - 任务1
# 当前子线程: Thread-2 - 任务2
# 总共用时0.0017192363739013672秒
# 结果: 1152921504606846976
# 结果: 1152921504606846976
```

### join()
如果要实现主线程和子线程的同步，我们必需使用join方法（代码如下所示)。
```py
import threading
import time


def long_time_task(i):
    print('当前子线程: {} 任务{}'.format(threading.current_thread().name, i))
    time.sleep(2)
    print("结果: {}".format(8 ** 20))


if __name__=='__main__':
    start = time.time()
    print('这是主线程：{}'.format(threading.current_thread().name))
    thread_list = []
    for i in range(1, 3):
        t = threading.Thread(target=long_time_task, args=(i, ))
        thread_list.append(t)

    for t in thread_list:
        t.start()

    for t in thread_list:
        t.join()

    end = time.time()
    print("总共用时{}秒".format((end - start)))

# 修改代码后的输出如下所示。这时你可以看到主线程在等子线程完成后才答应出总消耗时间(2秒)，比正常顺序执行代码(4秒)还是节省了不少时间。

# 输出：
# 这是主线程：MainThread
# 当前子线程: Thread - 1 任务1
# 当前子线程: Thread - 2 任务2
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 总共用时2.0166890621185303秒
```

当我们设置多线程时，主线程会创建多个子线程，在python中，默认情况下主线程和子线程独立运行互不干涉。<br>如果希望让主线程等待子线程实现线程的同步，我们需要使用join()方法。

### setDaemon()
如果我们希望一个<u>主线程结束时不再执行子线程</u>, <br>我们可以使用 **线程.setDaemon(True)** ，代码如下所示。

```py
import threading
import time


def long_time_task():
    print('当子线程: {}'.format(threading.current_thread().name))
    time.sleep(2)
    print("结果: {}".format(8 ** 20))


if __name__=='__main__':
    start = time.time()
    print('这是主线程：{}'.format(threading.current_thread().name))
    for i in range(5):
        t = threading.Thread(target=long_time_task, args=())
        t.setDaemon(True)   # setDaemon 设定主线程结束时不再执行子线程
        t.start()

    end = time.time()
    print("总共用时{}秒".format((end - start)))
```

### 通过继承Thread类重写run方法创建新进程

除了使用Thread()方法创建新的线程外，我们还可以通过继承Thread类重写run方法创建新的线程，这种方法更灵活。下例中我们自定义的类为MyThread, 随后我们通过该类的实例化创建了2个子线程。
```py
#-*- encoding:utf-8 -*-
import threading
import time


def long_time_task(i):
    time.sleep(2)
    return 8**20


class MyThread(threading.Thread):
    def __init__(self, func, args , name='', ):
        threading.Thread.__init__(self)
        self.func = func
        self.args = args
        self.name = name
        self.result = None

    def run(self):
        print('开始子进程{}'.format(self.name))
        self.result = self.func(self.args[0],)
        print("结果: {}".format(self.result))
        print('结束子进程{}'.format(self.name))


if __name__=='__main__':
    start = time.time()
    threads = []
    for i in range(1, 3):
        t = MyThread(long_time_task, (i,), str(i))
        threads.append(t)

    for t in threads:
        t.start()
    for t in threads:
        t.join()

    end = time.time()
    print("总共用时{}秒".format((end - start)))

# 输出:
# 开始子进程1
# 开始子进程2
# 结果: 1152921504606846976
# 结果: 1152921504606846976
# 结束子进程1
# 结束子进程2
# 总共用时2.005445718765259秒
```

## 不同线程间的数据共享 *

### lock()
一个进程所含的不同线程间共享内存，这就意味着任何一个变量都可以被任何一个线程修改，因此线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。<br>如果不同线程间有共享的变量，其中一个方法就是在修改前给其上一把锁lock，确保一次只有一个线程能修改它。

**threading.lock()** 方法可以轻易实现对一个共享变量的锁定，修改完后release供其它线程使用。

比如下例中账户余额balance是一个共享变量，使用lock可以使其不被改乱。
```py
# -*- coding: utf-8 -*

import threading


class Account:
    def __init__(self):
        self.balance = 0

    def add(self, lock):
        # 获得锁
        lock.acquire()
        for i in range(0, 100000):
            self.balance += 1
        # 释放锁
        lock.release()

    def delete(self, lock):
        # 获得锁
        lock.acquire()
        for i in range(0, 100000):
            self.balance -= 1
            # 释放锁
        lock.release()


if __name__ == "__main__":
    account = Account()
    lock = threading.Lock()

    thread_add = threading.Thread(target=account.add, args=(lock,), name='Add')  # 创建线程
    thread_delete = threading.Thread(target=account.delete, args=(lock,), name='Delete')

    
    thread_add.start()      # 启动线程
    thread_delete.start()

    
    thread_add.join()       # 等待线程结束
    thread_delete.join()

    print('The final balance is: {}'.format(account.balance))
```

### 使用消息队列queue
另一种实现不同线程间数据共享的方法就是使用消息队列queue。<br>不像列表，queue是线程安全的，可以放心使用，见下文。
<br>
使用queue队列通信示例
#### 经典的生产者和消费者模型

下例中创建了两个线程，一个负责生成，一个负责消费，所生成的产品存放在queue里，实现了不同线程间沟通。

```py
from queue import Queue
import random, threading, time


# 生产者类
class Producer(threading.Thread):
    def __init__(self, name, queue):
        threading.Thread.__init__(self, name=name)
        self.queue = queue

    def run(self):
        for i in range(1, 5):
            print("{} is producing {} to the queue!".format(self.getName(), i))
            self.queue.put(i)       # 往共享队列放数据
            time.sleep(random.randrange(10) / 5)
        print("%s finished!" % self.getName())


# 消费者类
class Consumer(threading.Thread):
    def __init__(self, name, queue):
        threading.Thread.__init__(self, name=name)
        self.queue = queue

    def run(self):
        for i in range(1, 5):
            val = self.queue.get()  # 从共享队列读数据
            print("{} is consuming {} in the queue.".format(self.getName(), val))
            time.sleep(random.randrange(10))
        print("%s finished!" % self.getName())


def main():
    queue = Queue()
    producer = Producer('Producer', queue)
    consumer = Consumer('Consumer', queue)

    producer.start()
    consumer.start()

    producer.join()
    consumer.join()
    print('All threads finished!')


if __name__ == '__main__':
    main()
```

队列queue的put方法可以将一个对象obj放入队列中。如果队列已满，此方法将阻塞至队列有空间可用为止。queue的get方法一次返回队列中的一个成员。如果队列为空，此方法将阻塞至队列中有成员可用为止。queue同时还自带emtpy(), full()等方法来判断一个队列是否为空或已满，但是这些方法并不可靠，因为多线程和多进程，在返回结果和使用结果之间，队列中可能添加/删除了成员。

# Python多进程和多线程哪个快?

由于GIL的存在，很多人认为Python多进程编程更快，针对多核CPU，理论上来说也是采用多进程更能有效利用资源。网上很多人已做过比较，我直接告诉你结论吧。
- 对 **CPU 密集型代码** (比如循环计算) - **多进程**效率更高
- 对 **IO 密集型代码** (比如文件操作，网络爬虫) - **多线程**效率更高。

为什么是这样呢？其实也不难理解。<br>对于IO密集型操作，大部分消耗时间其实是等待时间，在等待时间中CPU是不需要工作的，那你在此期间提供双CPU资源也是利用不上的，<br>相反对于CPU密集型代码，2个CPU干活肯定比一个CPU快很多。<br>那么为什么多线程会对IO密集型代码有用呢？这时因为python碰到等待会释放GIL供新的线程使用，实现了线程间的切换。

<br><br>
小结

本文总结了多进程和多线程的概念和区别, 并详细介绍如何使用python的multiprocess和threading模块进行多线程和多进程编程。我们还简单介绍了不同进程和线程间的通信和数据共享。如果您能熟练掌握本文中的所有知识点，那么你已经足以应付大部分面试和工作需求了。如果喜欢本文，就加入微信收藏常来看看吧。


-------------------------------------


[原文](http://python.jobbole.com/86069/)

# **Python 协程**：从yield/send到async/await

Python 由于众所周知的 GIL 的原因，导致其线程无法发挥多核的并行计算能力（当然，后来有了 `multiprocessing` ，可以实现多进程并行），显得比较鸡肋。

既然在 GIL 之下，同一时刻只能有一个线程在运行，
那么对于 ==CPU 密集的程序== 来说，<u>**线程之间的切换**</u> 开销就成了拖累。

而以 I/O 为瓶颈的程序正是 **协程所擅长的**：
<u>多任务并发（非并行），
每个任务在合适的时候 ==挂起==(发起I/O）和 ==恢复==(结束I/O)</u>


# Python中的协程发展历程
其大概经历了如下三个阶段：
1. 最初的生成器变形 **`yield`**/**`send`** 
2. 引入 **`@asyncio.coroutine`** 和 **`yield from`**
3. 在最近的 Python3.5 版本中引入 **`async`**/**`await`** 关键字，代替`@asyncio.coroutine` 和 `yield from `
<br>

## yield 生成器
先看一段普通的计算 **斐波那契续列** 的代码：
```py
def old_fib(n):
    res = [0] * n
    idx = 0
    a = 0
    b = 1
    while idx < n:
        res[idx] = b
        a, b = b, a + b
        idx += 1
    return res

for fib_res in old_fib(20):
    print(fib_res)
```
如果我们仅仅是需要拿到斐波那契序列的第n位，或者仅仅是希望依此产生斐波那契序列，那么上面这种传统方式就会比较耗费内存。

这时，yield就派上用场了。
```py
def fib(n):             # 生成器
    idx = 0
    a = 0
    b = 1
    while idx < n:
        yield b         # 保留 fib 函数的计算现场，暂停 fib 的计算并将 b 返回
        a, b = b, a + b
        idx += 1

# (迭代) next
for fib_res in fib(20): # fib(20): 生成器对象实例 
    print(fib_res)
```
==当一个函数中包含 yield 语句时， python 会自动将其识别为一个生成器==。
这时 `fib(20)` 并不会真正调用函数体，而是以函数体生成了一个生成器对象实例。

yield 在这里可以保留 fib 函数的计算现场，暂停 fib 的计算并将 b 返回。
<u>而将 fib 放入 for in 循环中时，每次循环都会调用 `next(fib(20))`，唤醒生成器，执行到下一个 yield 语句处，直到抛出 StopIteration 异常。</u>
此异常会被 for 循环捕获，导致跳出循环。
<br>

## Send（发送数据
从上面的程序中可以看到，目前只有数据从 fib(20) 中通过 yield 流向外面的 for 循环；
如果可以 <u>**向 fib(20) 发送数据**</u>，那不是就可以在 Python 中实现协程了嘛。

于是， Python 中的生成器有了 send 函数， yield 表达式也拥有了返回值。 

我们用这个特性，模拟一个额 **慢速斐波那契数列** 的计算：
```py
def stupid_fib(n):
    idx = 0
    a = 0
    b = 1
    while idx < n:
        sleep_cnt = yield b     # 接收外界发送的数据
        print('let me think {0} secs'.format(sleep_cnt))
        time.sleep(sleep_cnt)
        a, b = b, a + b
        idx += 1

N = 20
sfib = stupid_fib(N)
fib_res = next(sfib)    # sfib.send(None)
while True:
    print(fib_res)
    try:
        # 将一个随机的秒数发送给 sfib, 作为当前中断的 yield 表达式的返回值
        fib_res = sfib.send(random.uniform(0, 0.5)) 
    except StopIteration:
        break
```
==其中 `next(sfib)` 相当于 `sfib.send(None)`==，可以使得 sfib 运行至第一个 yield 处返回。
后续的 `sfib.send(random.uniform(0, 0.5))` 则将一个随机的秒数发送给 sfib, 作为当前中断的 yield 表达式的返回值。

这样，我们可以从“主”程序中控制协程计算斐波那契数列时的思考时间，协程可以返回给“主”程序计算结果， Perfect！
<br>

## yield from（重构生成器
yield from 用于 **复制生成器**，简单的，可以这么使用：
```py
def copy_fib(n):
    print('I am copy from fib')
    yield from fib(n)   # 复制生成器
    print('Copy end')

for fib_res in copy_fib(20):
    print(fib_res)
```
这种使用方式很简单，但远远不是 yield from 的全部。 

yield from 的作用还体现在：
<u>可以像一个 **管道** 一样将 send 信息传递给内层协程，并且处理好了各种异常情况。</u>

因此，对于 stupid_fib 也可以这样包装和使用：
```py
def copy_stupid_fib(n):
    print('I am copy from stupid fib')
    yield from stupid_fib(n)    # stupid_fib 能够接收数据的生成器
    print('Copy end')

N = 20
csfib = copy_stupid_fib(N)
fib_res = next(csfib)
while True:
    print(fib_res)
    try:
        # 传递数据
        fib_res = csfib.send(random.uniform(0, 0.5))
    except StopIteration:
        break
```
如果没有 yield from，这里的 copy_yield_from 将会特别复杂（因为要自己处理各种异常）。
<br>

### asyncio.coroutine
yield from 在 asyncio 模块中得以发扬光大。先看示例代码：
```py
@asyncio.coroutine
def smart_fib(n):
    idx = 0
    a = 0
    b = 1
    while idx < n:
        sleep_secs = random.uniform(0, 0.2) # 随机数
        # 通过 yield from ，将协程 asyncio.sleep 的控制权交给事件循环，然后挂起当前协程；
        # 之后，由事件循环决定何时唤醒 asyncio.sleep, 接着向后执行代码。
        yield from asyncio.sleep(sleep_secs)
        print('Smart one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        idx += 1

@asyncio.coroutine
def stupid_fib(n):
    idx = 0
    a = 0
    b = 1
    while idx < n:
        sleep_secs = random.uniform(0, 0.4)
        yield from asyncio.sleep(sleep_secs) 
        print('Stupid one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        idx += 1

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    tasks = [
        asyncio.async(smart_fib(10)),
        asyncio.async(stupid_fib(10)),
    ]
    loop.run_until_complete(asyncio.wait(tasks))
    print('All fib finished.')
    loop.close()
```
asyncio 是一个 <u>基于事件循环</u> 的 **实现异步 I/O 的模块**。

通过 yield from ，将协程 asyncio.sleep 的控制权交给事件循环，然后挂起当前协程；
之后，由事件循环决定何时唤醒 asyncio.sleep, 接着向后执行代码。

这样说可能比较抽象，好在 asyncio 是一个由 python 实现的模块，那么我们来看看 asyncio.sleep 中都做了些什么：
```py
@coroutine
def sleep(delay, result=None, *, loop=None):
    """ 
    Coroutine that completes after a given time (in seconds).
    """
    future = futures.Future(loop=loop)  # 创建 Future 对象，作为更内层的协程对象
    h = future._loop.call_later(        # 通过调用事件循环的 call_later 函数，注册了一个回调函数
        delay,future._set_result_unless_cancelled, result)
    try:
        return (yield from future)  # 将 Future 对象 通过 yield from 交给事件循环
    finally:
        h.cancel()
```
首先，sleep 创建了一个 Future 对象，作为更内层的协程对象，
通过 yield from 交给了事件循环；
其次，它通过调用事件循环的 call_later 函数，注册了一个回调函数。

通过查看 Future 类的源码，可以看到，
<u>**Future** 是一个实现了 `__iter__` 对象的生成器</u>：
```py
class Future:
    # blabla...
    def __iter__(self):
        if not self.done():
            self._blocking = True
            yield self          # This tells Task to wait for completion.
        assert self.done(), "yield from wasn't used with future"
        return self.result()    # May raise too.

```
那么当我们的协程 `yield from asyncio.sleep` 时，<u>事件循环</u> 其实是与 <u>Future 对象</u> 建立了联系。
- 每次事件循环调用 `send(None)` 时，其实都会传递到 Future 对象的 `__iter__` 函数调用；
- 而当 Future 尚未执行完毕的时候，就会 `yield self` ，也就意味着暂时挂起，等待下一次 `send(None)` 的唤醒。 

当我们包装一个 Future 对象产生一个 Task 对象时，在 Task 对象初始化中，就会调用 Future 的 send(None), 并且为 Future 设置好回调函数。
```py
class Task(futures.Future):
    # blabla...
    def _step(self, value=None, exc=None):
        # blabla...
        try:
            if exc is not None:
                result = coro.throw(exc)
            elif value is not None:
                result = coro.send(value)
            else:
                result = next(coro)
        # exception handle
        else:
            if isinstance(result, futures.Future):
                # Yielded Future must come from Future.__iter__().
                if result._blocking:
                    result._blocking = False
                    result.add_done_callback(self._wakeup)
        # blabla...

    def _wakeup(self, future):
        try:
            value = future.result()
        except Exception as exc:
            # This may also be a cancellation.
            self._step(None, exc)
        else:
            self._step(value, None)
        self = None  # Needed to break cycles when an exception occurs.
```
预设的时间过后，事件循环将调用 `Future._set_result_unless_cancelled`:
```py
class Future:
    #blabla...
    def _set_result_unless_cancelled(self, result):
        """Helper setting the result only if the future was not cancelled."""
        if self.cancelled():
            return
        self.set_result(result)

    def set_result(self, result):
        """Mark the future done and set its result.

        If the future is already done when this method is called, raises
        InvalidStateError.
        """
        if self._state != _PENDING:
            raise InvalidStateError('{}: {!r}'.format(self._state, self))
        self._result = result
        self._state = _FINISHED
        self._schedule_callbacks()
```
这将改变 Future 的状态，同时回调之前设定好的 Tasks._wakeup ；
在 _wakeup 中，将会再次调用 Tasks._step，这时，Future 的状态已经标记为完成，因此，将不再 yieldself ，
而 return 语句将会触发一个 StopIteration 异常，此异常将会被 Task._step 捕获用于设置 Task 的结果。
同时，整个 yield from 链条也将被唤醒，协程将继续往下执行。
<br>

## async 和 await
弄清楚了 asyncio.coroutine 和 yield from 之后，在 Python3.5 中引入的 async 和 await 就不难理解了：<u>可以将他们理解成 `asyncio.coroutine` / `yield from` 的完美替身。</u>

当然，从 Python 设计的角度来说， 
<u>async/await 让协程表面上独立于生成器而存在，将细节都隐藏于 asyncio 模块之下，语法更清晰明了。</u>
```py
async def smart_fib(n):
    idx = 0
    a = 0
    b = 1
    while idx < n:
        sleep_secs = random.uniform(0, 0.2)
        await asyncio.sleep(sleep_secs)
        print('Smart one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        idx += 1

async def stupid_fib(n):
    idx = 0
    a = 0
    b = 1
    while idx < n:
        sleep_secs = random.uniform(0, 0.4)
        await asyncio.sleep(sleep_secs)
        print('Stupid one think {} secs to get {}'.format(sleep_secs, b))
        a, b = b, a + b
        idx += 1

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    tasks = [
        asyncio.ensure_future(smart_fib(10)),
        asyncio.ensure_future(stupid_fib(10)),
    ]
    loop.run_until_complete(asyncio.wait(tasks))
    print('All fib finished.')
    loop.close()
```
<br>

## 总结
至此， Python 中的协程就介绍完毕了。
示例程序中都是以 sleep 为异步 I/O 的代表，
在实际项目中，可以使用协程异步的 **读写网络、读写文件、渲染界面** 等，
<u>而在等待协程完成的同时，CPU 还可以进行其他的计算。</u>
协程的作用正在于此。


# 简单应用
[出处：廖雪峰教程](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)

在学习异步 IO 模型前，我们先来了解协程。 
协程，又称微线程，纤程。英文名 Coroutine。 
协程的概念很早就提出来了，但直到最近几年才在某些语言（如 Lua）中得到广泛应用。 

- **子程序**，或者称为函数。
    - 在所有语言中都是层级调用，比如 A 调用 B ， B 在执行过程中又调用了 C ， C 执行完毕返回， B 执行完毕返回，最后是 A 执行完毕。 
    - 所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。 
    - 子程序调用总是一个入口，一次返回，调用顺序是明确的。

- **协程** 的调用和子程序不同。
    - <u>协程看上去也是子程序</u>，==但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。==
    - 注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点<u>类似 CPU 的中断</u>。

比如子程序 A 、 B ：
```py
def A():
    print('1')
    print('2')
    print('3')

def B():
    print('x')
    print('y')
    print('z')
```
假设由协程执行，在执行 A 的过程中，可以随时中断，去执行 B，B 也可能在执行过程中中断再去执行 A ，结果可能是：
```
1
2
x
y
3
z
```
但是在 A 中是没有调用 B 的，所以协程的调用比函数调用理解起来要难一些。 
看起来 A 、 B 的执行有点像多线程，
但协程的特点在于是 ==**一个线程执行**==。
- **那和多线程比，协程有何优势**？ 
    - 最大的优势就是协程<u>极高的执行效率</u>。
    因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 
    - 第二大优势就是<u>不需要多线程的锁机制</u>，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 
    <br>


- **因为协程是一个线程执行，那怎么利用多核 CPU 呢**？
最简单的方法是<u>多进程 + 协程</u>，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 
<br>

<u>**Python 对协程的支持是通过 生成器 generator 实现的**。</u>
在 generator 中，我们不但可以通过 for 循环来迭代，还可以不断调用 next() 函数获取由 yield 语句返回的下一个值。 
但是 ==Python 的 yield 不但可以返回一个值，它还可以接收调用者发出的参数==。
<br>


## 例子，生产者 - 消费者模型
是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。 
如果改用协程，生产者生产消息后，直接通过 yield 跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高：
```py
def consumer():         # 生成器
    ret = ''            # ret: 传递给外界的结果
    while True:
        n = yield ret   # n: 接收调用者传递的参数
        if not n:
            return
        print('[CONSUMER] Consuming %s...' % n)
        ret = '200 OK'

def produce(c):
    c.send(None)        # 启动 consumer 生成器
    n = 0
    while n < 5:
        n = n + 1
        print('[PRODUCER] Producing %s...' % n)
        # (真的很像函数调用，注意区分)
        ret = c.send(n) # consumer 生成器接收数据 返回处理后的值
        print('[PRODUCER] Consumer return: %s' % ret)
    c.close()           # 关闭 consumer 生成器

c = consumer()
produce(c)

# 执行结果：
# [PRODUCER] Producing 1...
# [CONSUMER] Consuming 1...
# [PRODUCER] Consumer return: 200 OK
# [PRODUCER] Producing 2...
# [CONSUMER] Consuming 2...
# [PRODUCER] Consumer return: 200 OK
# [PRODUCER] Producing 3...
# [CONSUMER] Consuming 3...
# [PRODUCER] Consumer return: 200 OK
# [PRODUCER] Producing 4...
# [CONSUMER] Consuming 4...
# [PRODUCER] Consumer return: 200 OK
# [PRODUCER] Producing 5...
# [CONSUMER] Consuming 5...
# [PRODUCER] Consumer return: 200 OK
```
注意： consumer 函数是一个 generator ，
把一个 consumer 传入 produce 后： 
- 首先调用 `c.send(None)` 启动生成器； 
- 然后，一旦生产了东西，
    - 通过 `c.send(n)` 切换到 consumer 执行； 
    - consumer 通过 `yield` 拿到消息，处理，又通过 `yield` 把结果传回； 
    - produce 拿到 consumer 处理的结果，继续生产下一条消息； 
- produce 决定不生产了，通过 `c.close()` 关闭 consumer ，整个过程结束。 

整个流程无锁，由一个线程执行， 
==produce 和 consumer 协作完成任务，所以称为“协程”==，而非线程的抢占式多任务。 

最后套用 Donald Knuth 的一句话总结协程的特点： <u>“子程序就是协程的一种特例。”</u>



-------------------------------------
